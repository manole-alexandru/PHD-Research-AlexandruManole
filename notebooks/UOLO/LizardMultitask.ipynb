{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","mount_file_id":"1yigO0GG0mTZllnNEuURuKWCY9WbF8WR3","authorship_tag":"ABX9TyMC0fT1Fi1980/22QbS4SMw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aLhxPc19b-Cb","executionInfo":{"status":"ok","timestamp":1759239267786,"user_tz":-180,"elapsed":70960,"user":{"displayName":"Alexandru Manole","userId":"16000231996200139655"}},"outputId":"a1ed2d6a-7026-45ce-ca53-853989530df5"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!mkdir dataset\n","!unzip -q /content/drive/MyDrive/MedicalData/LizardDataset.zip -d dataset"],"metadata":{"id":"sj80SNQyadKl","executionInfo":{"status":"ok","timestamp":1759239286600,"user_tz":-180,"elapsed":17788,"user":{"displayName":"Alexandru Manole","userId":"16000231996200139655"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import numpy as np\n","import torch\n","\n","CLASSES = ['Neutrophil', 'Epithelial', 'Lymphocyte', 'Plasma', 'Neutrophil', 'Connective tissue']\n","\n","def visualize_lizard_sample(image, inst_map, labels, bboxes, centroids, class_names=None):\n","    # Convert tensors to NumPy arrays\n","    img = image.detach().cpu()\n","    H, W = img.shape[1], img.shape[2]\n","    image_np = img.permute(1, 2, 0).numpy().astype(np.float32)\n","    image_np = (image_np * 255).astype(np.uint8)     # De-normalize for display if needed\n","\n","    inst_map_np = inst_map.detach().cpu().numpy()\n","    bboxes_np = bboxes.detach().cpu().numpy().astype(np.float32)\n","    cents_np = centroids.detach().cpu().numpy().astype(np.float32)\n","    labels_np = labels.detach().cpu().numpy().astype(np.int32)\n","\n","    fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n","\n","    # 1. Original Image\n","    axs[0].imshow(image_np)\n","    axs[0].set_title(\"Original Image\")\n","    axs[0].axis('off')\n","\n","    # 2. Instance Segmentation Map\n","    axs[1].imshow(inst_map_np, cmap='nipy_spectral')\n","    axs[1].set_title(\"Instance Segmentation\")\n","    axs[1].axis('off')\n","\n","    # 3. Bounding Boxes, Centroids, Class Labels\n","    axs[2].imshow(image_np)\n","    axs[2].set_title(\"BBoxes, Centroids, Labels\")\n","    axs[2].axis('off')\n","\n","    for i in range(len(labels)):\n","        x1, y1, x2, y2 = bboxes[i]\n","        cx, cy = centroids[i]\n","        class_id = labels[i] - 1\n","        if class_names:\n","            class_label = str(class_id.item()) # str(class_names[class_id])\n","\n","        # Draw bounding box\n","        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n","                                 linewidth=2, edgecolor='red', facecolor='none')\n","        axs[2].add_patch(rect)\n","\n","        # Draw centroid\n","        axs[2].plot(cx, cy, 'bo', markersize=4)\n","\n","        # Label text\n","        axs[2].text(x1, y1 - 5, class_label, color='yellow', fontsize=8, backgroundcolor='black')\n","\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"v_DF2qWwuBsj","executionInfo":{"status":"ok","timestamp":1759239290883,"user_tz":-180,"elapsed":4281,"user":{"displayName":"Alexandru Manole","userId":"16000231996200139655"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import os\n","from pathlib import Path\n","\n","import numpy as np\n","import scipy.io as sio\n","from PIL import Image\n","\n","import torch\n","from torch.utils.data import Dataset\n","\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","\n","class LizardDataset(Dataset):\n","    def __init__(self, image_root, label_root, transform=None, debug=False, padding_colour=(114, 114, 114)):\n","        self.image_paths, self.label_paths = [], []\n","        self.transform = transform\n","        self.debug = debug\n","\n","        # Gather all image-label pairs robustly\n","        for subdir in sorted(os.listdir(image_root)):\n","            subdir_path = os.path.join(image_root, subdir)\n","            if not (os.path.isdir(subdir_path) and subdir in ['lizard_images1', 'lizard_images2']):\n","                continue\n","\n","            # find first child directory deterministically\n","            child_dirs = sorted([d for d in os.listdir(subdir_path)\n","                                 if os.path.isdir(os.path.join(subdir_path, d))])\n","            if not child_dirs:\n","                continue\n","            image_folder = os.path.join(subdir_path, child_dirs[0])\n","\n","            for fname in sorted(os.listdir(image_folder)):\n","                if fname.lower().endswith(('.jpg', '.png', '.jpeg', '.tif', '.tiff')):\n","                    img_p = os.path.join(image_folder, fname)\n","                    mat_name = os.path.splitext(fname)[0] + '.mat'\n","                    lbl_p = os.path.join(label_root, 'Lizard_Labels', 'Labels', mat_name)\n","                    if os.path.exists(lbl_p):\n","                        self.image_paths.append(img_p)\n","                        self.label_paths.append(lbl_p)\n","\n","        self.aug = A.Compose(\n","            [\n","                A.LongestMaxSize(max_size=512),              # keep aspect ratio\n","                A.PadIfNeeded(min_height=512, min_width=512, position='center', border_mode=0, value=padding_colour, mask_value=0),\n","                # A.HorizontalFlip(p=0.5),\n","                # A.RandomBrightnessContrast(p=0.2),\n","                # A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n","                ToTensorV2(),\n","            ],\n","            bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bbox_labels']),\n","            # Keep keypoints if you still want them; you can also omit keypoints entirely and recompute from boxes\n","            keypoint_params=A.KeypointParams(format='xy', remove_invisible=False),\n","        )\n","\n","        print(f\"Found {len(self.image_paths)} images and {len(self.label_paths)} labels.\")\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        label_path = self.label_paths[idx]\n","\n","        image = np.array(Image.open(img_path).convert('RGB'))\n","        label = sio.loadmat(label_path)\n","\n","        inst_map = np.asarray(label['inst_map']).astype(np.uint8)\n","        bboxs = np.asarray(label['bbox']).squeeze()       # (N, 4): (y1, y2, x1, x2)\n","        centroids = np.asarray(label['centroid']).squeeze()\n","        classes = np.asarray(label['class']).squeeze()\n","\n","        # ensure consistent shapes for N==1\n","        if bboxs.ndim == 1 and bboxs.size == 4:\n","            bboxs = bboxs[None, :]\n","        if centroids.ndim == 1 and centroids.size == 2:\n","            centroids = centroids[None, :]\n","        if classes.ndim == 0:\n","            classes = classes[None]\n","\n","        bbox_list, bbox_labels, kpts = [], [], []\n","        for i in range(len(bboxs)):\n","            y1, y2, x1, x2 = bboxs[i].astype(float)\n","            bbox_list.append([x1, y1, x2, y2])  # pascal_voc: x_min,y_min,x_max,y_max\n","            bbox_labels.append(int(classes[i]))\n","            kpts.append((float(centroids[i][0]), float(centroids[i][1])))\n","\n","        if len(bbox_list) == 0:\n","            aug = self.aug(image=image, masks=[inst_map], bboxes=[], bbox_labels=[], keypoints=[])\n","            return (\n","                aug['image'],\n","                (\n","                    torch.tensor(aug['masks'][0], dtype=torch.long),\n","                    torch.empty(0, dtype=torch.int64),\n","                    torch.empty((0, 4), dtype=torch.float32),\n","                    torch.empty((0, 2), dtype=torch.float32),\n","                ),\n","            )\n","\n","        if self.debug:\n","            img_before_t = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n","            inst_before_t = torch.from_numpy(inst_map.astype(np.int64))\n","            boxes_before_t = torch.tensor(bbox_list, dtype=torch.float32) if len(bbox_list) else torch.empty((0,4),dtype=torch.float32)\n","            cents_before_t = torch.tensor(kpts, dtype=torch.float32) if len(kpts) else torch.empty((0,2),dtype=torch.float32)\n","            labels_before_t = torch.tensor(bbox_labels, dtype=torch.int64) if len(bbox_labels) else torch.empty((0,),dtype=torch.int64)\n","            print(img_before_t.shape)\n","            # labels_before_t = labels_before_t.numpy()\n","\n","            visualize_lizard_sample(img_before_t, inst_before_t, labels_before_t, boxes_before_t, cents_before_t, class_names=CLASSES)\n","\n","        aug = self.aug(\n","            image=image,\n","            masks=[inst_map],\n","            bboxes=bbox_list,\n","            bbox_labels=bbox_labels,\n","            keypoints=kpts,\n","        )\n","\n","        image_t = aug['image'] / 255.0\n","        inst_t = torch.tensor(aug['masks'][0], dtype=torch.long)\n","        bboxes_t = torch.tensor(aug['bboxes'], dtype=torch.float32)\n","\n","        # Recompute centroids from post-aug boxes (robust to filtering)\n","        if bboxes_t.numel() > 0:\n","            cx = (bboxes_t[:, 0] + bboxes_t[:, 2]) * 0.5\n","            cy = (bboxes_t[:, 1] + bboxes_t[:, 3]) * 0.5\n","            centroids_t = torch.stack([cx, cy], dim=1)\n","        else:\n","            centroids_t = torch.empty((0, 2), dtype=torch.float32)\n","\n","        labels_t = torch.tensor(aug['bbox_labels'], dtype=torch.int64)\n","\n","        # if self.debug:\n","        #    visualize_lizard_sample(image_t, inst_t, labels_t, bboxes_t, centroids_t, class_names=CLASSES)\n","\n","        return image_t, (inst_t, labels_t, bboxes_t, centroids_t)"],"metadata":{"id":"MZQdDQSOd3JH","executionInfo":{"status":"ok","timestamp":1759239371308,"user_tz":-180,"elapsed":1524,"user":{"displayName":"Alexandru Manole","userId":"16000231996200139655"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","def custom_collate_fn(batch):\n","    images = torch.stack([item[0] for item in batch])\n","    inst_maps = torch.stack([item[1][0] for item in batch])\n","    classes = [item[1][1] for item in batch]\n","    bboxes = [item[1][2] for item in batch]\n","    centroids = [item[1][3] for item in batch]\n","\n","    return images, (inst_maps, classes, bboxes, centroids)\n","\n","dataset = LizardDataset(\n","    image_root='/content/dataset/',\n","    label_root='/content/dataset/lizard_labels/',\n","    transform=None,\n","    debug = False,\n","    padding_colour=(114, 114, 114)\n",")\n","\n","dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=custom_collate_fn, num_workers=0) # workers has to be 0 while debugging"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bX-gtv1g4z89","executionInfo":{"status":"ok","timestamp":1759239377201,"user_tz":-180,"elapsed":10,"user":{"displayName":"Alexandru Manole","userId":"16000231996200139655"}},"outputId":"3c4444d0-fdb2-44df-a7f0-6475d27431c2"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 238 images and 238 labels.\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2323158094.py:48: UserWarning: Argument(s) 'value, mask_value' are not valid for transform PadIfNeeded\n","  A.PadIfNeeded(min_height=512, min_width=512, position='center', border_mode=0, value=padding_colour, mask_value=0),\n"]}]},{"cell_type":"markdown","source":["# Visualisation"],"metadata":{"id":"8lF8gPX0vxoR"}},{"cell_type":"code","source":["for batch_id, (images, (inst_maps, classes, bboxes, centroids)) in enumerate(dataloader):\n","    # print(f\"Image batch shape:     {images.shape}\")        # [B, 3, 256, 256]\n","    # print(f\"Inst map batch shape:  {inst_maps.shape}\")     # [B, 256, 256]\")\n","    # print(f\"Num images in batch:   {len(classes)}\")         # B\n","    # print(f\"Classes[0] shape:      {classes[0].shape}\")     # [N_0]\n","    # print(classes[0])\n","    #print(f\"BBoxes[0] shape:       {bboxes[0].shape}\")      # [N_0, 4]\n","    # print(f\"Centroids[0] shape:    {centroids[0].shape}\")   # [N_0, 2]\n","    visualize_lizard_sample(images[0], inst_maps[0], classes[0], bboxes[0], centroids[0], class_names=CLASSES)\n","    if (batch_id + 1) % 5 == 0:\n","      break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1kKOX5LL3mEd7vw3sOSH-0jnnScSHH6AO"},"id":"oa8VyBz6I4Z-","executionInfo":{"status":"ok","timestamp":1759230577522,"user_tz":-180,"elapsed":59118,"user":{"displayName":"Alexandru Manole","userId":"16000231996200139655"}},"outputId":"1cf65d02-3d74-422b-ae0e-1375ccd05a87"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["def visualize_lizard_sample_with_semantic_segmentation(image, inst_map, semantic_map, labels, bboxes, centroids, class_names=None):\n","    # Convert tensors to NumPy arrays (same style as your first function)\n","    img = image.detach().cpu()\n","    H, W = img.shape[1], img.shape[2]\n","    image_np = img.permute(1, 2, 0).numpy().astype(np.float32)\n","    image_np = (image_np * 255).astype(np.uint8)  # de-normalize for display if needed\n","\n","    inst_map_np = inst_map.detach().cpu().numpy()\n","    semantic_map_np = semantic_map.detach().cpu().numpy()\n","    bboxes_np = bboxes.detach().cpu().numpy().astype(np.float32)\n","    cents_np = centroids.detach().cpu().numpy().astype(np.float32)\n","    labels_np = labels.detach().cpu().numpy().astype(np.int32)\n","\n","    fig, axs = plt.subplots(1, 4, figsize=(24, 6))\n","\n","    # 1. Original Image\n","    axs[0].imshow(image_np)\n","    axs[0].set_title(\"Original Image\")\n","    axs[0].axis('off')\n","\n","    # 2. Instance Segmentation Map\n","    axs[1].imshow(inst_map_np, cmap='nipy_spectral')\n","    axs[1].set_title(\"Instance Segmentation\")\n","    axs[1].axis('off')\n","\n","    # 3. Semantic Segmentation Map\n","    axs[2].imshow(semantic_map_np, cmap='gray')\n","    axs[2].set_title(\"Semantic Segmentation\")\n","    axs[2].axis('off')\n","\n","    # 4. BBoxes, Centroids, Class Labels\n","    axs[3].imshow(image_np)\n","    axs[3].set_title(\"BBoxes, Centroids, Labels\")\n","    axs[3].axis('off')\n","\n","    for i in range(len(labels_np)):\n","        x1, y1, x2, y2 = bboxes_np[i]\n","        cx, cy = cents_np[i]\n","        class_id = labels_np[i] - 1\n","\n","        # Keep behavior similar to your first function; ensure class_label is always defined\n","        if class_names:\n","            # If you later want names, swap to: str(class_names[class_id])\n","            class_label = str(int(class_id))\n","        else:\n","            class_label = str(int(class_id))\n","\n","        # Draw bounding box\n","        rect = patches.Rectangle(\n","            (x1, y1), x2 - x1, y2 - y1,\n","            linewidth=2, edgecolor='red', facecolor='none'\n","        )\n","        axs[3].add_patch(rect)\n","\n","        # Draw centroid\n","        axs[3].plot(cx, cy, 'bo', markersize=4)\n","\n","        # Label text\n","        axs[3].text(x1, max(y1 - 5, 0), class_label,\n","                    color='yellow', fontsize=8, backgroundcolor='black')\n","\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"IQxBH1io7KCt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fetch a batch\n","for batch_id, (images, (inst_maps, classes, bboxes, centroids)) in enumerate(dataloader):\n","    semantic_maps = (inst_maps > 0).long()\n","\n","    visualize_lizard_sample_with_semantic_segmentation(images[0], inst_maps[0], semantic_maps[0], classes[0], bboxes[0], centroids[0], class_names=CLASSES)\n","    if (batch_id + 1) % 5 == 0:\n","        break"],"metadata":{"id":"-T0Ev43s8aua","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1uqUMdeZ0FcNiV9kwEGRPwZDwO_2c6U_4"},"executionInfo":{"status":"ok","timestamp":1759230604123,"user_tz":-180,"elapsed":26551,"user":{"displayName":"Alexandru Manole","userId":"16000231996200139655"}},"outputId":"408f8966-a998-4627-d957-bdbab7c9d06f"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import matplotlib.colors as mcolors\n","import numpy as np\n","import torch\n","\n","def visualize_lizard_sample_with_semantic_segmentation(image, inst_map, semantic_map, labels, bboxes, centroids, class_names=None):\n","    image_np = image.permute(1, 2, 0).cpu().numpy()\n","    image_np = (image_np * 255).astype(np.uint8)\n","\n","    class_names = ['Background'] + class_names\n","\n","    inst_map_np = inst_map.cpu().numpy()\n","    semantic_map_np = semantic_map.cpu().numpy()\n","    bboxes = bboxes.cpu().numpy()\n","    centroids = centroids.cpu().numpy()\n","    labels = labels.cpu().numpy()\n","\n","    fig, axs = plt.subplots(1, 4, figsize=(28, 6))\n","\n","    # 1. Original Image\n","    axs[0].imshow(image_np)\n","    axs[0].set_title(\"Original Image\")\n","    axs[0].axis('off')\n","\n","    # 2. Instance Segmentation\n","    axs[1].imshow(inst_map_np, cmap='nipy_spectral')\n","    axs[1].set_title(\"Instance Segmentation\")\n","    axs[1].axis('off')\n","\n","    # 3. Semantic Segmentation (Multi-class)\n","    # Use tab10 colormap (10 discrete colors), set bounds and ticks\n","    num_classes = 7\n","    cmap = plt.get_cmap('tab10', num_classes)\n","    bounds = np.arange(num_classes + 1) - 0.5\n","    norm = mcolors.BoundaryNorm(bounds, cmap.N)\n","\n","    im = axs[2].imshow(semantic_map_np, cmap=cmap, norm=norm)\n","    axs[2].set_title(\"Semantic Segmentation (0–6)\")\n","    axs[2].axis('off')\n","    cbar = plt.colorbar(im, ax=axs[2], ticks=range(num_classes), shrink=0.8)\n","    if class_names:\n","        cbar.ax.set_yticklabels([class_names[i] for i in range(num_classes)])\n","\n","    # 4. Bounding Boxes, Centroids, Labels\n","    axs[3].imshow(image_np)\n","    axs[3].set_title(\"BBoxes, Centroids, Labels\")\n","    axs[3].axis('off')\n","\n","    for i in range(len(labels)):\n","        x1, y1, x2, y2 = bboxes[i]\n","        cx, cy = centroids[i]\n","        class_id = labels[i] - 1\n","        if class_names:\n","            class_label = str(class_id) # str(class_names[class_id])\n","\n","        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n","                                 linewidth=2, edgecolor='red', facecolor='none')\n","        axs[3].add_patch(rect)\n","        axs[3].plot(cx, cy, 'bo', markersize=4)\n","        axs[3].text(x1, y1 - 5, class_label, color='yellow', fontsize=8, backgroundcolor='black')\n","\n","    plt.tight_layout()\n","    plt.show()\n"],"metadata":{"id":"QVWIfyGw9Ne8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for batch_id, (images, (inst_maps, classes, bboxes, centroids)) in enumerate(dataloader):\n","    for i in range(images.shape[0]):\n","        image = images[i]\n","        inst_map = inst_maps[i]\n","        class_ids = classes[i]\n","\n","        inst_map_np = inst_map.cpu().numpy()\n","        class_np = class_ids.cpu().numpy()\n","\n","        semantic_map = np.zeros_like(inst_map_np, dtype=np.uint8)\n","\n","        instance_ids = np.unique(inst_map_np)\n","        instance_ids = instance_ids[instance_ids != 0]\n","\n","        for j, inst_id in enumerate(instance_ids):\n","            if j < len(class_np):\n","                semantic_map[inst_map_np == inst_id] = class_np[j]\n","\n","        semantic_map = torch.tensor(semantic_map, dtype=torch.int64)\n","\n","        # Visualize\n","        visualize_lizard_sample_with_semantic_segmentation(\n","            image, inst_map, semantic_map, class_ids, bboxes[i], centroids[i], class_names=CLASSES\n","        )\n","\n","    if (batch_id + 1) % 5 == 0:\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1XUhKqVizFo_VfPIsXDsZdnNtrPj3qE1F"},"id":"uvX4u94Q9BLO","executionInfo":{"status":"ok","timestamp":1759230749465,"user_tz":-180,"elapsed":145285,"user":{"displayName":"Alexandru Manole","userId":"16000231996200139655"}},"outputId":"a8d29d1a-3b63-4abb-bd9b-e02a820213ae"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["for batch_id, (images, (inst_maps, classes, bboxes, centroids)) in enumerate(dataloader):\n","    for i in range(images.shape[0]):\n","        image = images[i]\n","        inst_map = inst_maps[i]\n","        class_ids = classes[i]\n","        box_list = bboxes[i]\n","\n","        inst_map_np = inst_map.cpu().numpy()\n","        class_np = class_ids.cpu().numpy()\n","        box_np = box_list.cpu().numpy()\n","\n","        semantic_map = np.zeros_like(inst_map_np, dtype=np.uint8)\n","\n","        # Go through all bounding boxes\n","        for j in range(len(box_np)):\n","            x1, y1, x2, y2 = box_np[j].astype(int)\n","            cls = class_np[j]\n","\n","            # Clip to valid range\n","            x1, y1 = max(x1, 0), max(y1, 0)\n","            x2, y2 = min(x2, inst_map_np.shape[1]), min(y2, inst_map_np.shape[0])\n","\n","            # Crop region from inst_map\n","            region = inst_map_np[y1:y2, x1:x2]\n","            instance_ids_in_box = np.unique(region)\n","            instance_ids_in_box = instance_ids_in_box[instance_ids_in_box != 0]\n","\n","            for inst_id in instance_ids_in_box:\n","                semantic_map[inst_map_np == inst_id] = cls\n","\n","        semantic_map = torch.tensor(semantic_map, dtype=torch.int64)\n","\n","        # Visualize\n","        visualize_lizard_sample_with_semantic_segmentation(\n","            image, inst_map, semantic_map, class_ids, box_list, centroids[i], class_names=CLASSES\n","        )\n","\n","    if (batch_id + 1) % 5 == 0:\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Lvy_0CtMLXYelgnGb2A07UQPUpk46KA9"},"id":"GlDmTUYCDuXR","executionInfo":{"status":"ok","timestamp":1759230866159,"user_tz":-180,"elapsed":116462,"user":{"displayName":"Alexandru Manole","userId":"16000231996200139655"}},"outputId":"476463a8-6fa0-4905-ebff-88d84101cd24"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import matplotlib.colors as mcolors\n","import numpy as np\n","import torch\n","\n","def compare_semantic_segmentations_with_detections(\n","    image,\n","    semantic_binary,\n","    semantic_from_instance,\n","    semantic_from_bbox,\n","    bboxes,\n","    centroids,\n","    class_labels,\n","    class_names=None\n","):\n","    class_names = ['Background'] + class_names\n","\n","    # Convert all tensors to NumPy arrays\n","    image_np = image.permute(1, 2, 0).cpu().numpy()\n","    image_np = (image_np * 255).astype(np.uint8)\n","\n","    sem_bin = semantic_binary.cpu().numpy()\n","    sem_inst = semantic_from_instance.cpu().numpy()\n","    sem_bbox = semantic_from_bbox.cpu().numpy()\n","    diff = (sem_inst != sem_bbox).astype(np.uint8)\n","\n","    bboxes = bboxes.cpu().numpy()\n","    centroids = centroids.cpu().numpy()\n","    class_labels = class_labels.cpu().numpy()\n","\n","    num_classes = 7\n","    cmap = plt.get_cmap('tab10', num_classes)\n","    bounds = np.arange(num_classes + 1) - 0.5\n","    norm = mcolors.BoundaryNorm(bounds, cmap.N)\n","\n","    fig, axs = plt.subplots(1, 5, figsize=(32, 6))\n","\n","    # 1. Binary segmentation\n","    axs[0].imshow(sem_bin, cmap='gray')\n","    axs[0].set_title(\"1. Binary Segmentation (0 = BG, 1 = FG)\")\n","    axs[0].axis('off')\n","\n","    # 2. Semantic from instance IDs\n","    im2 = axs[1].imshow(sem_inst, cmap=cmap, norm=norm)\n","    axs[1].set_title(\"2. Semantic from Instance IDs\")\n","    axs[1].axis('off')\n","    cbar2 = plt.colorbar(im2, ax=axs[1], ticks=range(num_classes), shrink=0.8)\n","    if class_names:\n","        cbar2.ax.set_yticklabels([class_names[i] for i in range(num_classes)])\n","\n","    # 3. Semantic from bounding boxes\n","    im3 = axs[2].imshow(sem_bbox, cmap=cmap, norm=norm)\n","    axs[2].set_title(\"3. Semantic from Bounding Boxes\")\n","    axs[2].axis('off')\n","    cbar3 = plt.colorbar(im3, ax=axs[2], ticks=range(num_classes), shrink=0.8)\n","    if class_names:\n","        cbar3.ax.set_yticklabels([class_names[i] for i in range(num_classes)])\n","\n","    # 4. Difference map\n","    axs[3].imshow(diff, cmap='gray', vmin=0, vmax=1)\n","    axs[3].set_title(\"4. Difference Map (White = mismatch)\")\n","    axs[3].axis('off')\n","\n","    # 5. Detections\n","    axs[4].imshow(image_np)\n","    axs[4].set_title(\"5. Detections (BBoxes + Labels + Centroids)\")\n","    axs[4].axis('off')\n","\n","    for i in range(len(bboxes)):\n","        x1, y1, x2, y2 = bboxes[i]\n","        cx, cy = centroids[i]\n","        cls_id = class_labels[i]\n","        label = str(cls_id)# class_names[cls_id] if class_names else str(cls_id)\n","\n","        # Draw bounding box\n","        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n","                                 linewidth=2, edgecolor='red', facecolor='none')\n","        axs[4].add_patch(rect)\n","\n","        # Draw centroid\n","        axs[4].plot(cx, cy, 'bo', markersize=4)\n","\n","        # Draw label\n","        axs[4].text(x1, y1 - 5, label, color='yellow', fontsize=8, backgroundcolor='black')\n","\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"2CA37siCHWU1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import matplotlib.colors as mcolors\n","import numpy as np\n","import torch\n","from scipy.ndimage import label as cc_label, generate_binary_structure\n","\n","def compare_semantic_segmentations_with_detections2(\n","    image,\n","    semantic_binary,\n","    semantic_from_instance,\n","    semantic_from_bbox,\n","    bboxes,\n","    centroids,\n","    class_labels,\n","    class_names=None,\n","    connectivity=8,      # 4 or 8 connectivity\n","    min_cc_area=1        # ignore components smaller than this (in pixels)\n","):\n","    if class_names is not None:\n","        class_names = ['Background'] + list(class_names)\n","\n","    # --- Tensors -> NumPy ---\n","    image_np = image.detach().cpu().permute(1, 2, 0).numpy()\n","    image_np = (np.clip(image_np, 0, 1) * 255).astype(np.uint8)\n","\n","    sem_bin = semantic_binary.detach().cpu().numpy().astype(bool)\n","    sem_inst = semantic_from_instance.detach().cpu().numpy()\n","    sem_bbox = semantic_from_bbox.detach().cpu().numpy()\n","    diff = (sem_inst != sem_bbox).astype(np.uint8)\n","\n","    bboxes = bboxes.detach().cpu().numpy()\n","    centroids = centroids.detach().cpu().numpy()\n","    class_labels = class_labels.detach().cpu().numpy()\n","\n","    # --- Connected components on the binary map ---\n","    structure = generate_binary_structure(2, 2 if connectivity == 8 else 1)\n","    labeled_cc, num_cc = cc_label(sem_bin, structure=structure)\n","\n","    # Optional area filtering\n","    if min_cc_area > 1 and num_cc > 0:\n","        areas = np.bincount(labeled_cc.ravel())[1:]  # skip background (0)\n","        keep_labels = np.where(areas >= min_cc_area)[0] + 1\n","        num_cc_kept = int(keep_labels.size)\n","    else:\n","        num_cc_kept = int(num_cc)\n","\n","    print(\"Length class labels\", len(class_labels))\n","    print(\"Length bboxes\", len(bboxes))\n","    print(f\"Connected components (all): {num_cc}\")\n","    if min_cc_area > 1:\n","        print(f\"Connected components (area ≥ {min_cc_area}): {num_cc_kept}\")\n","\n","    # --- Colormap for semantic maps ---\n","    if class_names is not None:\n","        num_classes = len(class_names)\n","    else:\n","        num_classes = int(max(sem_inst.max(), sem_bbox.max())) + 1 if sem_inst.size else 1\n","\n","    cmap_name = 'tab10' if num_classes <= 10 else 'tab20'\n","    cmap = plt.get_cmap(cmap_name, num_classes)\n","    bounds = np.arange(num_classes + 1) - 0.5\n","    norm = mcolors.BoundaryNorm(bounds, cmap.N)\n","\n","    # --- Figure ---\n","    fig, axs = plt.subplots(1, 5, figsize=(32, 6))\n","\n","    # 1. Binary segmentation (+ CC count)\n","    axs[0].imshow(sem_bin, cmap='gray')\n","    title_cc = f\"1. Binary Segmentation (CCs={num_cc_kept}\" + (f\", min_area={min_cc_area}\" if min_cc_area > 1 else \"\") + \")\"\n","    axs[0].set_title(title_cc)\n","    axs[0].axis('off')\n","\n","    # 2. Semantic from instance IDs\n","    im2 = axs[1].imshow(sem_inst, cmap=cmap, norm=norm)\n","    axs[1].set_title(\"2. Semantic from Instance IDs\")\n","    axs[1].axis('off')\n","    cbar2 = plt.colorbar(im2, ax=axs[1], ticks=range(num_classes), shrink=0.8)\n","    if class_names is not None:\n","        cbar2.ax.set_yticklabels(class_names)\n","\n","    # 3. Semantic from bounding boxes\n","    im3 = axs[2].imshow(sem_bbox, cmap=cmap, norm=norm)\n","    axs[2].set_title(\"3. Semantic from Bounding Boxes\")\n","    axs[2].axis('off')\n","    cbar3 = plt.colorbar(im3, ax=axs[2], ticks=range(num_classes), shrink=0.8)\n","    if class_names is not None:\n","        cbar3.ax.set_yticklabels(class_names)\n","\n","    # 4. Difference map\n","    axs[3].imshow(diff, cmap='gray', vmin=0, vmax=1)\n","    axs[3].set_title(\"4. Difference Map (White = mismatch)\")\n","    axs[3].axis('off')\n","\n","    # 5. Detections\n","    axs[4].imshow(image_np)\n","    axs[4].set_title(\"5. Detections (BBoxes + Labels + Centroids)\")\n","    axs[4].axis('off')\n","\n","    for i in range(len(bboxes)):\n","        x1, y1, x2, y2 = bboxes[i]\n","        cx, cy = centroids[i]\n","        cls_id = class_labels[i]\n","        label_txt = str(int(cls_id))  # or map via class_names if desired\n","\n","        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n","                                 linewidth=2, edgecolor='red', facecolor='none')\n","        axs[4].add_patch(rect)\n","        axs[4].plot(cx, cy, 'bo', markersize=4)\n","        axs[4].text(x1, max(y1 - 5, 0), label_txt, color='yellow',\n","                    fontsize=8, backgroundcolor='black')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # Optionally return counts if you want to use them programmatically\n","    return num_cc, num_cc_kept\n"],"metadata":{"id":"Pd-4bKxMDxCc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for batch_id, (images, (inst_maps, classes, bboxes, centroids)) in enumerate(dataloader):\n","    for i in range(images.shape[0]):\n","        image = images[i]\n","        inst_map = inst_maps[i]\n","        class_ids = classes[i]\n","        box_list = bboxes[i]\n","        centroid_list = centroids[i]\n","\n","        inst_map_np = inst_map.cpu().numpy()\n","        class_np = class_ids.cpu().numpy()\n","        box_np = box_list.cpu().numpy()\n","\n","        # 1. OLD semantic (binary)\n","        semantic_old = (inst_map > 0).long()\n","\n","        # 2. New semantic via bounding boxes\n","        semantic_from_bbox = np.zeros_like(inst_map_np, dtype=np.uint8)\n","        for j in range(len(box_np)):\n","            x1, y1, x2, y2 = box_np[j].astype(int)\n","            cls = class_np[j]\n","            x1, y1 = max(x1, 0), max(y1, 0)\n","            x2, y2 = min(x2, inst_map_np.shape[1]), min(y2, inst_map_np.shape[0])\n","            region = inst_map_np[y1:y2, x1:x2]\n","            instance_ids_in_box = np.unique(region)\n","            instance_ids_in_box = instance_ids_in_box[instance_ids_in_box != 0]\n","            for inst_id in instance_ids_in_box:\n","                semantic_from_bbox[inst_map_np == inst_id] = cls\n","        semantic_from_bbox = torch.tensor(semantic_from_bbox, dtype=torch.int64)\n","\n","        # 3. New semantic via instance order\n","        semantic_from_instance_order = np.zeros_like(inst_map_np, dtype=np.uint8)\n","        instance_ids = np.unique(inst_map_np)\n","        instance_ids = instance_ids[instance_ids != 0]\n","        for j, inst_id in enumerate(instance_ids):\n","            if j < len(class_np):\n","                semantic_from_instance_order[inst_map_np == inst_id] = class_np[j]\n","        semantic_from_instance_order = torch.tensor(semantic_from_instance_order, dtype=torch.int64)\n","\n","        # Convert inputs to tensors before passing\n","        box_tensor = torch.tensor(box_list, dtype=torch.float32)\n","        centroid_tensor = torch.tensor(centroid_list, dtype=torch.float32)\n","        class_tensor = torch.tensor(class_ids, dtype=torch.int64)\n","\n","        # Visualize comparison\n","        compare_semantic_segmentations_with_detections2(\n","            image,\n","            semantic_old,\n","            semantic_from_instance_order,\n","            semantic_from_bbox,\n","            box_tensor,\n","            centroid_tensor,\n","            class_tensor,\n","            CLASSES\n","        )\n","\n","    if (batch_id + 1) % 5 == 0:\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1IqIDTAEvDAqq3z8L52EVK9yFeZgGio46"},"id":"xtiDvtqGFcUn","outputId":"c2f06604-bb8f-4e02-f90f-46999ab2369a","executionInfo":{"status":"error","timestamp":1759230955444,"user_tz":-180,"elapsed":89047,"user":{"displayName":"Alexandru Manole","userId":"16000231996200139655"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["# Fetch a batch\n","for batch_id, (images, (inst_maps, classes, bboxes, centroids)) in enumerate(dataloader):\n","    semantic_maps = (inst_maps > 0).long()\n","\n","    visualize_lizard_sample_with_semantic_segmentation(images[0], inst_maps[0], semantic_maps[0], classes[0], bboxes[0], centroids[0], class_names=CLASSES)\n","    if (batch_id + 1) % 5 == 0:\n","        break"],"metadata":{"id":"5ALcikLuRa9F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Networks"],"metadata":{"id":"qlaAtJsESEs_"}},{"cell_type":"markdown","source":["## Segmentation"],"metadata":{"id":"HLYE0GzeTMN7"}},{"cell_type":"code","source":["import os\n","import json\n","\n","# ==== EXPERIMENT CONFIGURATION ====\n","EXP_NO = 0\n","LEARNING_RATE = 1e-3\n","BATCH_SIZE = 8\n","NUM_EPOCHS = 20\n","NUM_CLASSES = 7\n","K_FOLDS = 5\n","RANDOM_SEED = 42\n","NOTEBOOK_VERSION = 1\n","OPTIMIZER = \"adam\"  # \"adam\" or \"sgd\"\n","\n","# ==== DIRECTORY SETUP ====\n","ROOT_EXPERIMENT = \"/content/drive/MyDrive/MedicalData/Experiments/\"\n","EXPERIMENT_PATH = os.path.join(ROOT_EXPERIMENT, f\"exp_{EXP_NO}\")\n","os.makedirs(EXPERIMENT_PATH, exist_ok=True)\n","\n","# ==== SAVE CONFIGURATION ====\n","CONFIG = {\n","    \"EXP_NO\": EXP_NO,\n","    \"LEARNING_RATE\": LEARNING_RATE,\n","    \"BATCH_SIZE\": BATCH_SIZE,\n","    \"NUM_EPOCHS\": NUM_EPOCHS,\n","    \"NUM_CLASSES\": NUM_CLASSES,\n","    \"K_FOLDS\": K_FOLDS,\n","    \"RANDOM_SEED\": RANDOM_SEED,\n","    \"NOTEBOOK_VERSION\": NOTEBOOK_VERSION,\n","    \"OPTIMIZER\": OPTIMIZER.upper()\n","}\n","\n","with open(os.path.join(EXPERIMENT_PATH, \"config.json\"), \"w\") as f:\n","    json.dump(CONFIG, f, indent=4)\n","\n","print(f\"Experiment directory and config saved at: {EXPERIMENT_PATH}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BieF23OD0Aso","executionInfo":{"status":"ok","timestamp":1759223038087,"user_tz":-180,"elapsed":795,"user":{"displayName":"Alexandru Manole","userId":"16000231996200139655"}},"outputId":"dc69ce4d-dfb4-4189-824e-4a4dedcb0387"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Experiment directory and config saved at: /content/drive/MyDrive/MedicalData/Experiments/exp_0\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class UNet(nn.Module):\n","    def __init__(self, in_channels=3, out_classes=7):\n","        super(UNet, self).__init__()\n","\n","        def conv_block(in_c, out_c):\n","            return nn.Sequential(\n","                nn.Conv2d(in_c, out_c, 3, padding=1),\n","                nn.ReLU(inplace=True),\n","                nn.Conv2d(out_c, out_c, 3, padding=1),\n","                nn.ReLU(inplace=True)\n","            )\n","\n","        self.enc1 = conv_block(in_channels, 64)\n","        self.enc2 = conv_block(64, 128)\n","        self.enc3 = conv_block(128, 256)\n","\n","        self.pool = nn.MaxPool2d(2)\n","\n","        # Decoder blocks expect concatenated channels (enc + dec)\n","        self.dec3 = conv_block(256 + 128, 128)\n","        self.dec2 = conv_block(128 + 64, 64)\n","\n","        self.final = nn.Conv2d(64, out_classes, kernel_size=1)\n","\n","    def forward(self, x):\n","        # Encoder\n","        e1 = self.enc1(x)               # [B, 64, H, W]\n","        e2 = self.enc2(self.pool(e1))   # [B, 128, H/2, W/2]\n","        e3 = self.enc3(self.pool(e2))   # [B, 256, H/4, W/4]\n","\n","        # Decoder\n","        d3 = F.interpolate(e3, scale_factor=2, mode='bilinear', align_corners=False)\n","        d3 = self.dec3(torch.cat([d3, e2], dim=1))  # [B, 256+128, H/2, W/2] → [B, 128, H/2, W/2]\n","\n","        d2 = F.interpolate(d3, scale_factor=2, mode='bilinear', align_corners=False)\n","        d2 = self.dec2(torch.cat([d2, e1], dim=1))  # [B, 128+64, H, W] → [B, 64, H, W]\n","\n","        out = self.final(d2)  # [B, num_classes, H, W]\n","        return out"],"metadata":{"id":"1XQsp3G3SGYv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Subset, DataLoader\n","from sklearn.model_selection import KFold\n","\n","def compute_iou(preds, targets, num_classes):\n","    ious = []\n","    preds = preds.view(-1)\n","    targets = targets.view(-1)\n","    for cls in range(num_classes):\n","        pred_inds = preds == cls\n","        target_inds = targets == cls\n","        intersection = (pred_inds & target_inds).sum().item()\n","        union = (pred_inds | target_inds).sum().item()\n","        if union == 0:\n","            ious.append(float('nan'))\n","        else:\n","            ious.append(intersection / union)\n","    mean_iou = np.nanmean(ious)\n","    return mean_iou, ious\n","\n","def run_epoch(model, dataloader, optimizer, criterion, device, num_classes, is_train=True):\n","    model.train() if is_train else model.eval()\n","\n","    total_loss = 0\n","    total_iou = 0\n","    total_class_ious = np.zeros(num_classes)\n","    class_counts = np.zeros(num_classes)\n","    num_batches = 0\n","\n","    for images, (inst_maps, _, _, _) in dataloader:\n","        sem_seg_targets = (inst_maps > 0).long()\n","        images = images.to(device).float()\n","        targets = sem_seg_targets.to(device)\n","\n","        if is_train:\n","            optimizer.zero_grad()\n","\n","        outputs = model(images)\n","        loss = criterion(outputs, targets)\n","\n","        if is_train:\n","            loss.backward()\n","            optimizer.step()\n","\n","        total_loss += loss.item()\n","        num_batches += 1\n","\n","        with torch.no_grad():\n","            preds = torch.argmax(outputs, dim=1)\n","            mean_iou, class_ious = compute_iou(preds, targets, num_classes)\n","            total_iou += mean_iou\n","            for i, val in enumerate(class_ious):\n","                if not np.isnan(val):\n","                    total_class_ious[i] += val\n","                    class_counts[i] += 1\n","\n","    avg_loss = total_loss / num_batches\n","    avg_iou = total_iou / num_batches\n","    avg_class_ious = total_class_ious / np.maximum(class_counts, 1)\n","\n","    return avg_loss, avg_iou, avg_class_ious\n","\n","def plot_metrics(metrics, folder_path):\n","    epochs = np.arange(1, len(metrics[\"train_loss\"]) + 1)\n","    plt.figure()\n","    plt.plot(epochs, metrics[\"train_loss\"], label=\"Train Loss\")\n","    plt.plot(epochs, metrics[\"val_loss\"], label=\"Val Loss\")\n","    plt.plot(epochs, metrics[\"train_miou\"], label=\"Train mIoU\")\n","    plt.plot(epochs, metrics[\"val_miou\"], label=\"Val mIoU\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Value\")\n","    plt.legend()\n","    plt.title(\"Training Metrics\")\n","    plt.savefig(os.path.join(folder_path, \"metric_plot.png\"))\n","    plt.close()\n","\n","def plot_class_iou_hist(class_ious, folder_path):\n","    final_class_ious = np.array(class_ious[-1])\n","    plt.figure()\n","    plt.bar(range(len(final_class_ious)), final_class_ious)\n","    plt.xlabel(\"Class\")\n","    plt.ylabel(\"IoU\")\n","    plt.title(\"Final Epoch Class IoUs\")\n","    plt.savefig(os.path.join(folder_path, \"class_iou_hist.png\"))\n","    plt.close()\n","\n","def cross_validate(model_class, dataset, device, output_dir):\n","    kfold = KFold(n_splits=K_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):\n","        print(f\"\\n===== Fold {fold+1}/{K_FOLDS} =====\")\n","\n","        fold_dir = os.path.join(output_dir, f\"fold_{fold}\")\n","        os.makedirs(fold_dir, exist_ok=True)\n","\n","        train_loader = DataLoader(Subset(dataset, train_ids), batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn)\n","        val_loader = DataLoader(Subset(dataset, val_ids), batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)\n","\n","        model = model_class().to(device)\n","\n","        if OPTIMIZER.lower() == \"adam\":\n","            optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","        elif OPTIMIZER.lower() == \"sgd\":\n","            optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n","        else:\n","            raise ValueError(f\"Unsupported optimizer: {OPTIMIZER}\")\n","\n","        criterion = nn.CrossEntropyLoss()\n","        best_val_miou = -1\n","        best_model_path = os.path.join(fold_dir, \"model_best.pth\")\n","\n","        metrics = {\n","            \"train_loss\": [],\n","            \"val_loss\": [],\n","            \"train_miou\": [],\n","            \"val_miou\": [],\n","            \"val_class_ious\": []\n","        }\n","\n","        for epoch in range(NUM_EPOCHS):\n","            train_loss, train_miou, _ = run_epoch(model, train_loader, optimizer, criterion, device, NUM_CLASSES, is_train=True)\n","            val_loss, val_miou, val_class_ious = run_epoch(model, val_loader, optimizer, criterion, device, NUM_CLASSES, is_train=False)\n","\n","            metrics[\"train_loss\"].append(train_loss)\n","            metrics[\"val_loss\"].append(val_loss)\n","            metrics[\"train_miou\"].append(train_miou)\n","            metrics[\"val_miou\"].append(val_miou)\n","            metrics[\"val_class_ious\"].append(val_class_ious)\n","\n","            print(f\"[Epoch {epoch+1}] Train Loss: {train_loss:.4f} | Train mIoU: {train_miou:.4f}\")\n","            print(f\"           Val Loss: {val_loss:.4f} | Val mIoU: {val_miou:.4f}\")\n","\n","            if val_miou > best_val_miou:\n","                best_val_miou = val_miou\n","                torch.save(model.state_dict(), best_model_path)\n","\n","        np.save(os.path.join(fold_dir, \"metrics.npy\"), metrics)\n","        plot_metrics(metrics, fold_dir)\n","        plot_class_iou_hist(metrics[\"val_class_ious\"], fold_dir)"],"metadata":{"id":"zAzTZCAoShZl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = UNet(in_channels=3, out_classes=NUM_CLASSES)\n","\n","if OPTIMIZER.lower() == \"adam\":\n","    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","elif OPTIMIZER.lower() == \"sgd\":\n","    optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n","else:\n","    assert False\n","\n","cross_validate(\n","    model_class=lambda: UNet(in_channels=3, out_classes=NUM_CLASSES),\n","    dataset=dataset,\n","    device='cuda',\n","    output_dir=EXPERIMENT_PATH\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VYPB72eYTITH","executionInfo":{"status":"ok","timestamp":1757958116646,"user_tz":-180,"elapsed":1361242,"user":{"displayName":"Alexandru Manole","userId":"16000231996200139655"}},"outputId":"08de9539-5d26-44d8-86be-54bdf96877c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","===== Fold 1/5 =====\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-60827223.py:82: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  inst_map = torch.tensor(aug['masks'][0], dtype=torch.int32)\n"]},{"output_type":"stream","name":"stdout","text":["[Epoch 1] Train Loss: 0.6094 | Train mIoU: 0.3885\n","           Val Loss: 0.3907 | Val mIoU: 0.4210\n","[Epoch 2] Train Loss: 0.3881 | Train mIoU: 0.4436\n","           Val Loss: 0.3299 | Val mIoU: 0.4913\n","[Epoch 3] Train Loss: 0.3510 | Train mIoU: 0.5264\n","           Val Loss: 0.3065 | Val mIoU: 0.5520\n","[Epoch 4] Train Loss: 0.3285 | Train mIoU: 0.5526\n","           Val Loss: 0.2896 | Val mIoU: 0.5801\n","[Epoch 5] Train Loss: 0.3238 | Train mIoU: 0.5663\n","           Val Loss: 0.3010 | Val mIoU: 0.5946\n","[Epoch 6] Train Loss: 0.3179 | Train mIoU: 0.5722\n","           Val Loss: 0.2796 | Val mIoU: 0.5956\n","[Epoch 7] Train Loss: 0.3049 | Train mIoU: 0.5940\n","           Val Loss: 0.2858 | Val mIoU: 0.5831\n","[Epoch 8] Train Loss: 0.3149 | Train mIoU: 0.5856\n","           Val Loss: 0.2714 | Val mIoU: 0.6003\n","[Epoch 9] Train Loss: 0.3096 | Train mIoU: 0.5859\n","           Val Loss: 0.2836 | Val mIoU: 0.5684\n","[Epoch 10] Train Loss: 0.3169 | Train mIoU: 0.5799\n","           Val Loss: 0.2817 | Val mIoU: 0.6255\n","[Epoch 11] Train Loss: 0.2923 | Train mIoU: 0.6051\n","           Val Loss: 0.2629 | Val mIoU: 0.5960\n","[Epoch 12] Train Loss: 0.2868 | Train mIoU: 0.6153\n","           Val Loss: 0.3002 | Val mIoU: 0.5497\n","[Epoch 13] Train Loss: 0.2952 | Train mIoU: 0.6029\n","           Val Loss: 0.2778 | Val mIoU: 0.6002\n","[Epoch 14] Train Loss: 0.2824 | Train mIoU: 0.6202\n","           Val Loss: 0.2568 | Val mIoU: 0.6402\n","[Epoch 15] Train Loss: 0.2747 | Train mIoU: 0.6301\n","           Val Loss: 0.2630 | Val mIoU: 0.5998\n","[Epoch 16] Train Loss: 0.2747 | Train mIoU: 0.6301\n","           Val Loss: 0.2487 | Val mIoU: 0.6336\n","[Epoch 17] Train Loss: 0.2792 | Train mIoU: 0.6287\n","           Val Loss: 0.2512 | Val mIoU: 0.6519\n","[Epoch 18] Train Loss: 0.2789 | Train mIoU: 0.6322\n","           Val Loss: 0.2532 | Val mIoU: 0.6095\n","[Epoch 19] Train Loss: 0.2771 | Train mIoU: 0.6364\n","           Val Loss: 0.2502 | Val mIoU: 0.6292\n","[Epoch 20] Train Loss: 0.2695 | Train mIoU: 0.6369\n","           Val Loss: 0.2426 | Val mIoU: 0.6522\n","\n","===== Fold 2/5 =====\n","[Epoch 1] Train Loss: 0.7573 | Train mIoU: 0.3887\n","           Val Loss: 0.3843 | Val mIoU: 0.4172\n","[Epoch 2] Train Loss: 0.3705 | Train mIoU: 0.4658\n","           Val Loss: 0.4351 | Val mIoU: 0.5740\n","[Epoch 3] Train Loss: 0.3529 | Train mIoU: 0.5087\n","           Val Loss: 0.3278 | Val mIoU: 0.5782\n","[Epoch 4] Train Loss: 0.3252 | Train mIoU: 0.5577\n","           Val Loss: 0.3153 | Val mIoU: 0.5957\n","[Epoch 5] Train Loss: 0.3218 | Train mIoU: 0.5688\n","           Val Loss: 0.3098 | Val mIoU: 0.5606\n","[Epoch 6] Train Loss: 0.3203 | Train mIoU: 0.5667\n","           Val Loss: 0.3571 | Val mIoU: 0.6108\n","[Epoch 7] Train Loss: 0.3139 | Train mIoU: 0.5784\n","           Val Loss: 0.3044 | Val mIoU: 0.5688\n","[Epoch 8] Train Loss: 0.3113 | Train mIoU: 0.5791\n","           Val Loss: 0.2966 | Val mIoU: 0.6150\n","[Epoch 9] Train Loss: 0.2974 | Train mIoU: 0.5979\n","           Val Loss: 0.3090 | Val mIoU: 0.6273\n","[Epoch 10] Train Loss: 0.3011 | Train mIoU: 0.5910\n","           Val Loss: 0.3124 | Val mIoU: 0.6275\n","[Epoch 11] Train Loss: 0.3021 | Train mIoU: 0.5932\n","           Val Loss: 0.2834 | Val mIoU: 0.5887\n","[Epoch 12] Train Loss: 0.2942 | Train mIoU: 0.5940\n","           Val Loss: 0.2847 | Val mIoU: 0.5904\n","[Epoch 13] Train Loss: 0.3016 | Train mIoU: 0.5976\n","           Val Loss: 0.2861 | Val mIoU: 0.6215\n","[Epoch 14] Train Loss: 0.2951 | Train mIoU: 0.6067\n","           Val Loss: 0.2796 | Val mIoU: 0.6205\n","[Epoch 15] Train Loss: 0.2881 | Train mIoU: 0.6098\n","           Val Loss: 0.2756 | Val mIoU: 0.6235\n","[Epoch 16] Train Loss: 0.2866 | Train mIoU: 0.6160\n","           Val Loss: 0.2808 | Val mIoU: 0.6264\n","[Epoch 17] Train Loss: 0.2879 | Train mIoU: 0.6110\n","           Val Loss: 0.2753 | Val mIoU: 0.6091\n","[Epoch 18] Train Loss: 0.2879 | Train mIoU: 0.6124\n","           Val Loss: 0.2755 | Val mIoU: 0.5920\n","[Epoch 19] Train Loss: 0.2839 | Train mIoU: 0.6139\n","           Val Loss: 0.2824 | Val mIoU: 0.6335\n","[Epoch 20] Train Loss: 0.2834 | Train mIoU: 0.6147\n","           Val Loss: 0.2828 | Val mIoU: 0.6253\n","\n","===== Fold 3/5 =====\n","[Epoch 1] Train Loss: 0.9631 | Train mIoU: 0.3745\n","           Val Loss: 0.4250 | Val mIoU: 0.4100\n","[Epoch 2] Train Loss: 0.4038 | Train mIoU: 0.4187\n","           Val Loss: 0.4002 | Val mIoU: 0.4226\n","[Epoch 3] Train Loss: 0.3598 | Train mIoU: 0.4660\n","           Val Loss: 0.3620 | Val mIoU: 0.5179\n","[Epoch 4] Train Loss: 0.3332 | Train mIoU: 0.5431\n","           Val Loss: 0.3456 | Val mIoU: 0.5548\n","[Epoch 5] Train Loss: 0.3304 | Train mIoU: 0.5578\n","           Val Loss: 0.3458 | Val mIoU: 0.5805\n","[Epoch 6] Train Loss: 0.3224 | Train mIoU: 0.5600\n","           Val Loss: 0.3439 | Val mIoU: 0.5824\n","[Epoch 7] Train Loss: 0.3133 | Train mIoU: 0.5847\n","           Val Loss: 0.3316 | Val mIoU: 0.5553\n","[Epoch 8] Train Loss: 0.3076 | Train mIoU: 0.5779\n","           Val Loss: 0.3444 | Val mIoU: 0.5704\n","[Epoch 9] Train Loss: 0.3142 | Train mIoU: 0.5811\n","           Val Loss: 0.3225 | Val mIoU: 0.5710\n","[Epoch 10] Train Loss: 0.2985 | Train mIoU: 0.5909\n","           Val Loss: 0.3193 | Val mIoU: 0.5988\n","[Epoch 11] Train Loss: 0.3101 | Train mIoU: 0.5883\n","           Val Loss: 0.3239 | Val mIoU: 0.5728\n","[Epoch 12] Train Loss: 0.3031 | Train mIoU: 0.5828\n","           Val Loss: 0.3301 | Val mIoU: 0.5944\n","[Epoch 13] Train Loss: 0.2978 | Train mIoU: 0.5904\n","           Val Loss: 0.3142 | Val mIoU: 0.5898\n","[Epoch 14] Train Loss: 0.2925 | Train mIoU: 0.6035\n","           Val Loss: 0.3143 | Val mIoU: 0.5782\n","[Epoch 15] Train Loss: 0.2921 | Train mIoU: 0.6040\n","           Val Loss: 0.3147 | Val mIoU: 0.5829\n","[Epoch 16] Train Loss: 0.2860 | Train mIoU: 0.6075\n","           Val Loss: 0.3181 | Val mIoU: 0.6066\n","[Epoch 17] Train Loss: 0.2870 | Train mIoU: 0.6110\n","           Val Loss: 0.3058 | Val mIoU: 0.6179\n","[Epoch 18] Train Loss: 0.2887 | Train mIoU: 0.6053\n","           Val Loss: 0.3018 | Val mIoU: 0.5909\n","[Epoch 19] Train Loss: 0.2827 | Train mIoU: 0.6141\n","           Val Loss: 0.3106 | Val mIoU: 0.5718\n","[Epoch 20] Train Loss: 0.2886 | Train mIoU: 0.6128\n","           Val Loss: 0.2943 | Val mIoU: 0.5960\n","\n","===== Fold 4/5 =====\n","[Epoch 1] Train Loss: 0.8439 | Train mIoU: 0.3800\n","           Val Loss: 0.4151 | Val mIoU: 0.4186\n","[Epoch 2] Train Loss: 0.3975 | Train mIoU: 0.4270\n","           Val Loss: 0.3620 | Val mIoU: 0.4663\n","[Epoch 3] Train Loss: 0.3398 | Train mIoU: 0.5290\n","           Val Loss: 0.3175 | Val mIoU: 0.5559\n","[Epoch 4] Train Loss: 0.3227 | Train mIoU: 0.5665\n","           Val Loss: 0.3100 | Val mIoU: 0.5617\n","[Epoch 5] Train Loss: 0.3217 | Train mIoU: 0.5806\n","           Val Loss: 0.3225 | Val mIoU: 0.5475\n","[Epoch 6] Train Loss: 0.3143 | Train mIoU: 0.5844\n","           Val Loss: 0.3026 | Val mIoU: 0.5650\n","[Epoch 7] Train Loss: 0.3018 | Train mIoU: 0.5982\n","           Val Loss: 0.3001 | Val mIoU: 0.6017\n","[Epoch 8] Train Loss: 0.3031 | Train mIoU: 0.5974\n","           Val Loss: 0.3097 | Val mIoU: 0.5555\n","[Epoch 9] Train Loss: 0.2927 | Train mIoU: 0.6061\n","           Val Loss: 0.2837 | Val mIoU: 0.5987\n","[Epoch 10] Train Loss: 0.2811 | Train mIoU: 0.6218\n","           Val Loss: 0.2746 | Val mIoU: 0.6162\n","[Epoch 11] Train Loss: 0.2811 | Train mIoU: 0.6261\n","           Val Loss: 0.2818 | Val mIoU: 0.6069\n","[Epoch 12] Train Loss: 0.2775 | Train mIoU: 0.6328\n","           Val Loss: 0.2671 | Val mIoU: 0.6180\n","[Epoch 13] Train Loss: 0.2682 | Train mIoU: 0.6388\n","           Val Loss: 0.2633 | Val mIoU: 0.6301\n","[Epoch 14] Train Loss: 0.2653 | Train mIoU: 0.6424\n","           Val Loss: 0.2618 | Val mIoU: 0.6371\n","[Epoch 15] Train Loss: 0.2618 | Train mIoU: 0.6464\n","           Val Loss: 0.2791 | Val mIoU: 0.6495\n","[Epoch 16] Train Loss: 0.2665 | Train mIoU: 0.6407\n","           Val Loss: 0.2620 | Val mIoU: 0.6445\n","[Epoch 17] Train Loss: 0.2602 | Train mIoU: 0.6517\n","           Val Loss: 0.2622 | Val mIoU: 0.6529\n","[Epoch 18] Train Loss: 0.2608 | Train mIoU: 0.6513\n","           Val Loss: 0.2587 | Val mIoU: 0.6434\n","[Epoch 19] Train Loss: 0.2636 | Train mIoU: 0.6489\n","           Val Loss: 0.2584 | Val mIoU: 0.6475\n","[Epoch 20] Train Loss: 0.2564 | Train mIoU: 0.6565\n","           Val Loss: 0.2634 | Val mIoU: 0.6032\n","\n","===== Fold 5/5 =====\n","[Epoch 1] Train Loss: 0.7623 | Train mIoU: 0.3760\n","           Val Loss: 0.3985 | Val mIoU: 0.4183\n","[Epoch 2] Train Loss: 0.3646 | Train mIoU: 0.4808\n","           Val Loss: 0.3376 | Val mIoU: 0.5451\n","[Epoch 3] Train Loss: 0.3228 | Train mIoU: 0.5669\n","           Val Loss: 0.3716 | Val mIoU: 0.4913\n","[Epoch 4] Train Loss: 0.3318 | Train mIoU: 0.5658\n","           Val Loss: 0.3676 | Val mIoU: 0.4911\n","[Epoch 5] Train Loss: 0.3255 | Train mIoU: 0.5636\n","           Val Loss: 0.3093 | Val mIoU: 0.5720\n","[Epoch 6] Train Loss: 0.3009 | Train mIoU: 0.5970\n","           Val Loss: 0.3979 | Val mIoU: 0.4965\n","[Epoch 7] Train Loss: 0.3134 | Train mIoU: 0.5801\n","           Val Loss: 0.3098 | Val mIoU: 0.5303\n","[Epoch 8] Train Loss: 0.3003 | Train mIoU: 0.5940\n","           Val Loss: 0.3036 | Val mIoU: 0.5846\n","[Epoch 9] Train Loss: 0.3017 | Train mIoU: 0.5949\n","           Val Loss: 0.3008 | Val mIoU: 0.6084\n","[Epoch 10] Train Loss: 0.2950 | Train mIoU: 0.6105\n","           Val Loss: 0.2906 | Val mIoU: 0.6050\n","[Epoch 11] Train Loss: 0.3104 | Train mIoU: 0.5960\n","           Val Loss: 0.3053 | Val mIoU: 0.5573\n","[Epoch 12] Train Loss: 0.2924 | Train mIoU: 0.6023\n","           Val Loss: 0.2994 | Val mIoU: 0.6036\n","[Epoch 13] Train Loss: 0.2869 | Train mIoU: 0.6158\n","           Val Loss: 0.2888 | Val mIoU: 0.6149\n","[Epoch 14] Train Loss: 0.2781 | Train mIoU: 0.6260\n","           Val Loss: 0.2824 | Val mIoU: 0.6234\n","[Epoch 15] Train Loss: 0.2817 | Train mIoU: 0.6245\n","           Val Loss: 0.2862 | Val mIoU: 0.5892\n","[Epoch 16] Train Loss: 0.2724 | Train mIoU: 0.6347\n","           Val Loss: 0.2770 | Val mIoU: 0.6166\n","[Epoch 17] Train Loss: 0.2696 | Train mIoU: 0.6355\n","           Val Loss: 0.2729 | Val mIoU: 0.6329\n","[Epoch 18] Train Loss: 0.2663 | Train mIoU: 0.6437\n","           Val Loss: 0.2871 | Val mIoU: 0.6456\n","[Epoch 19] Train Loss: 0.2696 | Train mIoU: 0.6408\n","           Val Loss: 0.2791 | Val mIoU: 0.6332\n","[Epoch 20] Train Loss: 0.2604 | Train mIoU: 0.6502\n","           Val Loss: 0.2761 | Val mIoU: 0.6210\n"]}]},{"cell_type":"markdown","source":["## Detection"],"metadata":{"id":"PobDURT_TQG6"}},{"cell_type":"code","source":["import torchvision\n","from torchvision.models.detection import maskrcnn_resnet50_fpn\n","\n","def get_maskrcnn_model(num_classes=6):\n","    model = maskrcnn_resnet50_fpn(pretrained=True)\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","\n","    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n","        in_features, num_classes\n","    )\n","    model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(\n","        model.roi_heads.mask_predictor.conv5_mask.in_channels,\n","        256,\n","        num_classes\n","    )\n","    return model"],"metadata":{"id":"BDVlfduOSIRM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Helper: build torchvision targets from your batch ---\n","def _boxes_to_int(b, H, W):\n","    # clamp and convert to int pixel indices\n","    x1 = max(0, min(W - 1, int(round(float(b[0])))))\n","    y1 = max(0, min(H - 1, int(round(float(b[1])))))\n","    x2 = max(0, min(W - 1, int(round(float(b[2])))))\n","    y2 = max(0, min(H - 1, int(round(float(b[3])))))\n","    # ensure proper ordering\n","    x1, x2 = (x1, x2) if x1 <= x2 else (x2, x1)\n","    y1, y2 = (y1, y2) if y1 <= y2 else (y2, y1)\n","    return x1, y1, x2, y2\n","\n","def _derive_instance_masks_from_inst_map(inst_map_t, boxes_t, labels_t):\n","    \"\"\"\n","    Convert a per-pixel inst_map (H,W) with unique instance IDs into a stack of instance masks (N,H,W),\n","    one per bbox/label. We map each bbox to the most frequent non-zero instance id inside the box.\n","    \"\"\"\n","    H, W = inst_map_t.shape[-2], inst_map_t.shape[-1]\n","    inst_np = inst_map_t.cpu().numpy()  # int64/long\n","    masks = []\n","    kept_boxes = []\n","    kept_labels = []\n","\n","    for i in range(len(boxes_t)):\n","        x1, y1, x2, y2 = _boxes_to_int(boxes_t[i], H, W)\n","        if x2 <= x1 or y2 <= y1:\n","            continue  # degenerate box after aug\n","        crop = inst_np[y1:y2+1, x1:x2+1]\n","        # pick the dominant nonzero instance id in this crop\n","        vals, counts = np.unique(crop, return_counts=True)\n","        nonzero = [(v, c) for v, c in zip(vals, counts) if v != 0]\n","        if not nonzero:\n","            # fall back: if no instance id in the crop, skip this target\n","            continue\n","        inst_id = max(nonzero, key=lambda t: t[1])[0]\n","        mask = (inst_np == inst_id)\n","        masks.append(torch.from_numpy(mask.astype(np.uint8)))\n","        kept_boxes.append(boxes_t[i])\n","        kept_labels.append(labels_t[i])\n","\n","    if len(masks) == 0:\n","        # Return properly shaped empties\n","        return (\n","            torch.empty((0, H, W), dtype=torch.uint8),\n","            torch.empty((0, 4), dtype=boxes_t.dtype, device=boxes_t.device),\n","            torch.empty((0,), dtype=labels_t.dtype, device=labels_t.device),\n","        )\n","    masks_t = torch.stack(masks, dim=0).to(inst_map_t.device)\n","    boxes_t = torch.stack(kept_boxes, dim=0) if isinstance(kept_boxes[0], torch.Tensor) else torch.tensor(kept_boxes, device=inst_map_t.device)\n","    labels_t = torch.stack(kept_labels, dim=0) if isinstance(kept_labels[0], torch.Tensor) else torch.tensor(kept_labels, device=inst_map_t.device)\n","    return masks_t, boxes_t, labels_t\n","\n","\n","def build_targets_from_batch(inst_maps, classes, bboxes):\n","    \"\"\"\n","    Convert your collated batch to torchvision-style targets: a list of dicts,\n","    each with keys: 'boxes' (FloatTensor[N,4]), 'labels' (Int64Tensor[N]),\n","    'masks' (UInt8Tensor[N,H,W]).\n","    \"\"\"\n","    batch_targets = []\n","    B = inst_maps.shape[0]\n","    for i in range(B):\n","        inst_map_i = inst_maps[i]                           # (H,W) long\n","        labels_i   = classes[i]                             # (Ni,)\n","        boxes_i    = bboxes[i]                              # (Ni,4)\n","\n","        # Ensure correct dtypes/devices\n","        labels_i = labels_i.to(dtype=torch.int64, device=inst_map_i.device)\n","        boxes_i  = boxes_i.to(dtype=torch.float32, device=inst_map_i.device)\n","\n","        masks_i, boxes_i, labels_i = _derive_instance_masks_from_inst_map(inst_map_i, boxes_i, labels_i)\n","\n","        tgt = {\n","            \"boxes\":  boxes_i,                              # [Ni,4] x1,y1,x2,y2\n","            \"labels\": labels_i,                             # [Ni]\n","            \"masks\":  masks_i,                              # [Ni,H,W] uint8 (0/1)\n","        }\n","        batch_targets.append(tgt)\n","    return batch_targets"],"metadata":{"id":"xvn-xeRwsc66"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Add this helper ---\n","def _sanitize_targets(targets, num_classes):\n","    sane = []\n","    for t in targets:\n","        boxes  = t[\"boxes\"]\n","        labels = torch.tensor(t[\"labels\"], dtype=torch.int64) - 1\n","        masks  = t.get(\"masks\", None)\n","\n","        # valid labels: 1..num_classes-1\n","        valid = (labels >= 0) & (labels <= (num_classes - 1))\n","        if valid.numel() == 0:\n","            sane.append({\"boxes\": boxes.new_zeros((0,4)), \"labels\": labels.new_zeros((0,), dtype=torch.int64),\n","                         \"masks\": masks.new_zeros((0, *masks.shape[-2:])) if masks is not None else None})\n","            continue\n","\n","        if valid.sum().item() != labels.numel():\n","            # drop invalid ones\n","            boxes  = boxes[valid]\n","            labels = labels[valid]\n","            masks  = masks[valid] if masks is not None and masks.numel() > 0 else masks\n","\n","        # drop degenerate boxes (area <= 0) to be safe\n","        if boxes.numel() > 0:\n","            x1, y1, x2, y2 = boxes.unbind(1)\n","            good = (x2 > x1) & (y2 > y1) & torch.isfinite(boxes).all(dim=1)\n","            boxes  = boxes[good]\n","            labels = labels[good]\n","            masks  = masks[good] if masks is not None and masks.numel() > 0 else masks\n","\n","        sane.append({\"boxes\": boxes, \"labels\": labels, \"masks\": masks})\n","    return sane\n","\n","def _get_num_classes(model):\n","    bp = model.roi_heads.box_predictor\n","    lin = getattr(bp, \"cls_score\", None) or getattr(bp, \"cls_logits\", None)\n","    return lin.out_features"],"metadata":{"id":"sAnwOiKVy548"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import math\n","from collections import defaultdict\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","# --- Training loop ---\n","def train_maskrcnn(\n","    model,\n","    dataloader,\n","    num_epochs=10,\n","    lr=1e-3,\n","    weight_decay=1e-4,\n","    clip_grad_norm=0.0,           # set >0 to enable grad clipping\n","    use_amp=True,\n","    lr_scheduler_fn=None,         # e.g., lambda opt: torch.optim.lr_scheduler.StepLR(opt, step_size=5, gamma=0.1)\n","    device=None,\n","    print_every=20,\n","):\n","    \"\"\"\n","    Trains Mask R-CNN using your dataloader (images, (inst_maps, classes, bboxes, _)).\n","    Returns the trained model.\n","    \"\"\"\n","    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","    model.train()\n","\n","    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n","    scaler = torch.cuda.amp.GradScaler(enabled=use_amp and (device == \"cuda\"))\n","    scheduler = lr_scheduler_fn(optimizer) if lr_scheduler_fn else None\n","\n","    for epoch in range(1, num_epochs + 1):\n","        running = defaultdict(float)\n","        count = 0\n","\n","        for step, batch in enumerate(dataloader, start=1):\n","            images, (inst_maps, classes, bboxes, _centroids) = batch\n","\n","            # Move images to device\n","            images = [img.to(device) for img in images]  # list[Tensor[C,H,W]]\n","            inst_maps = inst_maps.to(device)\n","\n","            # Build torchvision targets\n","            targets = build_targets_from_batch(inst_maps, classes, bboxes)\n","\n","            num_classes_model = _get_num_classes(model)\n","            targets = _sanitize_targets(targets, num_classes=num_classes_model)\n","\n","            if any(t[\"labels\"].numel() for t in targets):\n","              all_labels = torch.cat([t[\"labels\"].cpu() for t in targets if t[\"labels\"].numel()], 0)\n","              # pass\n","              print(\"Label min/max:\", int(all_labels.min()), int(all_labels.max()),\n","                    \"| num_classes:\", num_classes_model)\n","\n","            # If every image ended up with 0 instances (rare), skip the step to avoid NaNs\n","            if all(t[\"boxes\"].numel() == 0 for t in targets):\n","                continue\n","\n","            optimizer.zero_grad(set_to_none=True)\n","\n","            with torch.cuda.amp.autocast(enabled=scaler.is_enabled()):\n","                loss_dict = model(images, targets)  # training mode => returns losses\n","                print(loss_dict)\n","                loss = sum(loss_dict.values())\n","\n","            if torch.isfinite(loss):\n","                if scaler.is_enabled():\n","                    scaler.scale(loss).backward()\n","                    if clip_grad_norm and clip_grad_norm > 0:\n","                        scaler.unscale_(optimizer)\n","                        nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n","                    scaler.step(optimizer)\n","                    scaler.update()\n","                else:\n","                    loss.backward()\n","                    if clip_grad_norm and clip_grad_norm > 0:\n","                        nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n","                    optimizer.step()\n","            else:\n","                # Skip bad batch\n","                continue\n","\n","            # Bookkeeping\n","            count += 1\n","            running[\"loss\"] += float(loss.detach().cpu())\n","            for k, v in loss_dict.items():\n","                running[k] += float(v.detach().cpu())\n","\n","            if step % print_every == 0:\n","                denom = max(1, count)\n","                msg = f\"[Epoch {epoch:02d} | Step {step:04d}] \"\n","                msg += \" \".join([f\"{k}: {running[k]/denom:.4f}\" for k in [\"loss\", *sorted([kk for kk in loss_dict.keys()])]])\n","                print(msg)\n","\n","        # Epoch end\n","        if count > 0:\n","            avg_loss = running[\"loss\"] / count\n","            print(f\"Epoch {epoch:02d} done. Avg loss: {avg_loss:.4f}\")\n","        else:\n","            print(f\"Epoch {epoch:02d} done. (no usable batches)\")\n","\n","        if scheduler is not None:\n","            scheduler.step()\n","\n","    return model"],"metadata":{"id":"2a_BpcsXsV-R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for step, batch in enumerate(dataloader, start=1):\n","    images, (inst_maps, classes, bboxes, _centroids) = batch\n","    targets = build_targets_from_batch(inst_maps, classes, bboxes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":376},"id":"In8uxYKKGo7y","executionInfo":{"status":"error","timestamp":1759229809930,"user_tz":-180,"elapsed":50478,"user":{"displayName":"Alexandru Manole","userId":"16000231996200139655"}},"outputId":"a760b675-6808-411c-c6b7-8601133b0bce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3887134637.py:123: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  inst_t = torch.tensor(aug['masks'][0], dtype=torch.long)\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-651711649.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minst_maps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_centroids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_targets_from_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst_maps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-3770855488.py\u001b[0m in \u001b[0;36mbuild_targets_from_batch\u001b[0;34m(inst_maps, classes, bboxes)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mboxes_i\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mboxes_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minst_map_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mmasks_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_derive_instance_masks_from_inst_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst_map_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         tgt = {\n","\u001b[0;32m/tmp/ipython-input-3770855488.py\u001b[0m in \u001b[0;36m_derive_instance_masks_from_inst_map\u001b[0;34m(inst_map_t, boxes_t, labels_t)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0minst_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minst_np\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minst_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mkept_boxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mkept_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["model = get_maskrcnn_model(num_classes=6)\n","trained_model = train_maskrcnn(model, dataloader, num_epochs=20, lr=5e-4, weight_decay=1e-4, clip_grad_norm=1.0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"2mM7P-mxsYf5","executionInfo":{"status":"error","timestamp":1759231660311,"user_tz":-180,"elapsed":568,"user":{"displayName":"Alexandru Manole","userId":"16000231996200139655"}},"outputId":"69c98587-97a7-4116-e03c-0e26cf8ea01c"},"execution_count":null,"outputs":[{"output_type":"error","ename":"AcceleratorError","evalue":"CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1566282438.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_maskrcnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_maskrcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-3806465105.py\u001b[0m in \u001b[0;36mtrain_maskrcnn\u001b[0;34m(model, dataloader, num_epochs, lr, weight_decay, clip_grad_norm, use_amp, lr_scheduler_fn, device, print_every)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \"\"\"\n\u001b[1;32m     25\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1367\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1353\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m                     )\n\u001b[0;32m-> 1355\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1356\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}]},{"cell_type":"markdown","source":["# Error"],"metadata":{"id":"oiyuNXzDJFBH"}},{"cell_type":"code","source":["import os, torch\n","os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # better stack traces next time\n","print(\"CUDA available:\", torch.cuda.is_available())\n","x = torch.randn(1, device=\"cuda\")  # should succeed"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5BGQztn2JeLE","executionInfo":{"status":"ok","timestamp":1759227879333,"user_tz":-180,"elapsed":235,"user":{"displayName":"Alexandru Manole","userId":"16000231996200139655"}},"outputId":"49c108d2-7b3f-47ae-fe47-cca298f72f02"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA available: True\n"]}]},{"cell_type":"code","source":["# Recreate model fresh\n","model = get_maskrcnn_model(num_classes=6)\n","\n","# CPU smoke test (ensures weights and shapes are fine)\n","_ = model.eval()\n","dummy = [torch.rand(3, 256, 256)]  # float in [0,1]\n","with torch.no_grad():\n","    out = model(dummy)  # should run on CPU\n","\n","# Now move to GPU\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ocMlsH_0J0IF","executionInfo":{"status":"ok","timestamp":1759227939120,"user_tz":-180,"elapsed":3501,"user":{"displayName":"Alexandru Manole","userId":"16000231996200139655"}},"outputId":"054b00f4-5f85-46c4-fbb7-57081ec2fe17"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170M/170M [00:00<00:00, 236MB/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["MaskRCNN(\n","  (transform): GeneralizedRCNNTransform(\n","      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n","  )\n","  (backbone): BackboneWithFPN(\n","    (body): IntermediateLayerGetter(\n","      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","      (relu): ReLU(inplace=True)\n","      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (layer1): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (1): FrozenBatchNorm2d(256, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer2): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(512, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer3): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(1024, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (4): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (5): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer4): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(2048, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","    )\n","    (fpn): FeaturePyramidNetwork(\n","      (inner_blocks): ModuleList(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (2): Conv2dNormActivation(\n","          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","      )\n","      (layer_blocks): ModuleList(\n","        (0-3): 4 x Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        )\n","      )\n","      (extra_blocks): LastLevelMaxPool()\n","    )\n","  )\n","  (rpn): RegionProposalNetwork(\n","    (anchor_generator): AnchorGenerator()\n","    (head): RPNHead(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (1): ReLU(inplace=True)\n","        )\n","      )\n","      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n","  (roi_heads): RoIHeads(\n","    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n","    (box_head): TwoMLPHead(\n","      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n","      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n","    )\n","    (box_predictor): FastRCNNPredictor(\n","      (cls_score): Linear(in_features=1024, out_features=6, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=24, bias=True)\n","    )\n","    (mask_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(14, 14), sampling_ratio=2)\n","    (mask_head): MaskRCNNHeads(\n","      (0): Conv2dNormActivation(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","      )\n","      (1): Conv2dNormActivation(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","      )\n","      (2): Conv2dNormActivation(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","      )\n","      (3): Conv2dNormActivation(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","      )\n","    )\n","    (mask_predictor): MaskRCNNPredictor(\n","      (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n","      (relu): ReLU(inplace=True)\n","      (mask_fcn_logits): Conv2d(256, 6, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["import torch, torchvision\n","print(\"torch:\", torch.__version__)\n","print(\"torchvision:\", torchvision.__version__)\n","print(\"cuda available:\", torch.cuda.is_available())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y6hWdW0rKLjL","executionInfo":{"status":"ok","timestamp":1759227993163,"user_tz":-180,"elapsed":44,"user":{"displayName":"Alexandru Manole","userId":"16000231996200139655"}},"outputId":"9cadeb45-7868-4dca-b4e8-f3d56189bfc7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch: 2.8.0+cu126\n","torchvision: 0.23.0+cu126\n","cuda available: True\n"]}]},{"cell_type":"code","source":["for step, batch in enumerate(dataloader, start=1):\n","    images, (inst_maps, classes, bboxes, _centroids) = batch\n","\n","    images = [img.to(device).float() for img in images]\n","    for i in range(len(images)):\n","        images[i].clamp_(0, 1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W8H5k7MBKOmz","executionInfo":{"status":"ok","timestamp":1759228305949,"user_tz":-180,"elapsed":13168,"user":{"displayName":"Alexandru Manole","userId":"16000231996200139655"}},"outputId":"33fd7b82-1c32-437e-dfdd-d6e83bffe785"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3887134637.py:123: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  inst_t = torch.tensor(aug['masks'][0], dtype=torch.long)\n"]}]},{"cell_type":"code","source":["model = get_maskrcnn_model(num_classes=6)\n","trained_model = train_maskrcnn(model, dataloader, num_epochs=20, lr=5e-4, weight_decay=1e-4, clip_grad_norm=1.0, device=\"cpu\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LPa-5PyrKktQ","outputId":"c8c993f3-f7bf-40fb-89f8-a231ff9e4edb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3806465105.py:30: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=use_amp and (device == \"cuda\"))\n","/tmp/ipython-input-3887134637.py:123: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  inst_t = torch.tensor(aug['masks'][0], dtype=torch.long)\n","/tmp/ipython-input-384491701.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels = torch.tensor(t[\"labels\"], dtype=torch.int64) - 1\n"]},{"output_type":"stream","name":"stdout","text":["Label min/max: 0 5 | num_classes: 6\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3806465105.py:62: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=scaler.is_enabled()):\n"]},{"output_type":"stream","name":"stdout","text":["{'loss_classifier': tensor(1.8837, grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.3697, grad_fn=<DivBackward0>), 'loss_mask': tensor(1.5287, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_objectness': tensor(8.4098, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.6213, grad_fn=<DivBackward0>)}\n","Label min/max: 0 5 | num_classes: 6\n","{'loss_classifier': tensor(0.9100, grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.3237, grad_fn=<DivBackward0>), 'loss_mask': tensor(0.8419, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_objectness': tensor(0.6624, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.4849, grad_fn=<DivBackward0>)}\n","Label min/max: 0 5 | num_classes: 6\n","{'loss_classifier': tensor(1.1601, grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.3644, grad_fn=<DivBackward0>), 'loss_mask': tensor(0.6513, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_objectness': tensor(0.3756, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(1.7435, grad_fn=<DivBackward0>)}\n","Label min/max: 1 5 | num_classes: 6\n","{'loss_classifier': tensor(1.0582, grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.1705, grad_fn=<DivBackward0>), 'loss_mask': tensor(0.6021, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_objectness': tensor(0.8155, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.5424, grad_fn=<DivBackward0>)}\n","Label min/max: 0 5 | num_classes: 6\n"]}]},{"cell_type":"markdown","source":["# Transform dataset to YOLO"],"metadata":{"id":"bU9w9F5A29Ie"}},{"cell_type":"code","source":["IMAGE_ROOT = '/content/dataset/'  # root that contains lizard_images1/ lizard_images2/\n","LABEL_ROOT = '/content/dataset/lizard_labels/'   # root that contains Lizard_Labels/Labels/*.mat\n","\n","# Where to write the YOLO dataset in Drive\n","OUT_DIR = \"/content/drive/MyDrive/yolo_lizard_512/\"\n","DATASET_NAME = \"yolo_lizard_512\"\n","\n","# Export image/mask format\n","IMG_EXT = \".jpeg\"\n","MASK_EXT = \".jpeg\"   # consider \".png\" if you prefer lossless\n","JPEG_QUALITY = 95\n","\n","# Export size must match your aug below\n","TARGET_SIZE = 512\n","\n","import os\n","os.makedirs(OUT_DIR, exist_ok=True)\n","os.makedirs(f\"{OUT_DIR}/images\", exist_ok=True)\n","os.makedirs(f\"{OUT_DIR}/labels\", exist_ok=True)\n","os.makedirs(f\"{OUT_DIR}/masks\", exist_ok=True)\n","print(\"Output dirs ready:\", OUT_DIR)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V_Naoewg3CoT","executionInfo":{"status":"ok","timestamp":1759240144622,"user_tz":-180,"elapsed":7,"user":{"displayName":"Alexandru Manole","userId":"16000231996200139655"}},"outputId":"3af4c912-5525-4826-9c95-9a7b47458cf6"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Output dirs ready: /content/drive/MyDrive/yolo_lizard_512/\n"]}]},{"cell_type":"code","source":["import os\n","from pathlib import Path\n","import numpy as np\n","import scipy.io as sio\n","from PIL import Image\n","\n","import torch\n","from torch.utils.data import Dataset\n","\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","# (Optional) quick visualizer for sanity checks later\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","\n","def draw_boxes(ax, boxes, labels=None, color=\"lime\", lw=2):\n","    for i, b in enumerate(boxes):\n","        x1, y1, x2, y2 = [float(v) for v in b]\n","        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=lw, edgecolor=color, facecolor='none')\n","        ax.add_patch(rect)\n","        if labels is not None:\n","            ax.text(x1, y1-2, str(labels[i]), fontsize=8, color=color, bbox=dict(fc=\"black\", alpha=0.3, pad=1))\n","\n","class LizardDataset(Dataset):\n","    def __init__(self, image_root, label_root, transform=None, debug=False, padding_colour=(114, 114, 114)):\n","        self.image_paths, self.label_paths = [], []\n","        self.transform = transform\n","        self.debug = debug\n","\n","        print(image_root)\n","\n","        for subdir in sorted(os.listdir(image_root)):\n","            subdir_path = os.path.join(image_root, subdir)\n","            if not (os.path.isdir(subdir_path) and subdir in ['lizard_images1', 'lizard_images2']):\n","                continue\n","\n","            child_dirs = sorted([d for d in os.listdir(subdir_path)\n","                                 if os.path.isdir(os.path.join(subdir_path, d))])\n","            if not child_dirs:\n","                continue\n","            image_folder = os.path.join(subdir_path, child_dirs[0])\n","\n","            for fname in sorted(os.listdir(image_folder)):\n","                if fname.lower().endswith(('.jpg', '.png', '.jpeg', '.tif', '.tiff')):\n","                    img_p = os.path.join(image_folder, fname)\n","                    mat_name = os.path.splitext(fname)[0] + '.mat'\n","                    lbl_p = os.path.join(label_root, 'Lizard_Labels', 'Labels', mat_name)\n","                    if os.path.exists(lbl_p):\n","                        self.image_paths.append(img_p)\n","                        self.label_paths.append(lbl_p)\n","\n","        self.aug = A.Compose(\n","            [\n","                A.LongestMaxSize(max_size=TARGET_SIZE),\n","                A.PadIfNeeded(min_height=TARGET_SIZE, min_width=TARGET_SIZE,\n","                              position='center', border_mode=0,\n","                              value=(114,114,114), mask_value=0),\n","                ToTensorV2(),\n","            ],\n","            bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bbox_labels']),\n","            keypoint_params=A.KeypointParams(format='xy', remove_invisible=False),\n","        )\n","\n","        print(f\"Found {len(self.image_paths)} images and {len(self.label_paths)} labels.\")\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        label_path = self.label_paths[idx]\n","\n","        image = np.array(Image.open(img_path).convert('RGB'))\n","        label = sio.loadmat(label_path)\n","\n","        inst_map = np.asarray(label['inst_map']).astype(np.uint8)\n","        bboxs = np.asarray(label['bbox']).squeeze()       # (N, 4): (y1, y2, x1, x2)\n","        centroids = np.asarray(label['centroid']).squeeze()\n","        classes = np.asarray(label['class']).squeeze()\n","\n","        if bboxs.ndim == 1 and bboxs.size == 4:\n","            bboxs = bboxs[None, :]\n","        if centroids.ndim == 1 and centroids.size == 2:\n","            centroids = centroids[None, :]\n","        if classes.ndim == 0:\n","            classes = classes[None]\n","\n","        bbox_list, bbox_labels, kpts = [], [], []\n","        for i in range(len(bboxs)):\n","            y1, y2, x1, x2 = bboxs[i].astype(float)\n","            bbox_list.append([x1, y1, x2, y2])  # x_min,y_min,x_max,y_max\n","            bbox_labels.append(int(classes[i]))\n","            kpts.append((float(centroids[i][0]), float(centroids[i][1])))\n","\n","        aug = self.aug(\n","            image=image,\n","            masks=[inst_map],\n","            bboxes=bbox_list,\n","            bbox_labels=bbox_labels,\n","            keypoints=kpts,\n","        )\n","\n","        image_t = aug['image'] / 255.0          # C,H,W in [0,1]\n","        inst_t = torch.tensor(aug['masks'][0], dtype=torch.long)\n","        bboxes_t = torch.tensor(aug['bboxes'], dtype=torch.float32)\n","        labels_t = torch.tensor(aug['bbox_labels'], dtype=torch.int64)\n","\n","        return image_t, (inst_t, labels_t, bboxes_t), Path(img_path).stem"],"metadata":{"id":"CHrjlb3M3W9c","executionInfo":{"status":"ok","timestamp":1759240123377,"user_tz":-180,"elapsed":9,"user":{"displayName":"Alexandru Manole","userId":"16000231996200139655"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["import shutil\n","from tqdm import tqdm\n","import math\n","\n","def _clamp_boxes(boxes, w, h):\n","    if boxes.numel() == 0:\n","        return boxes\n","    boxes[:, 0] = boxes[:, 0].clamp(0, w-1)  # x1\n","    boxes[:, 2] = boxes[:, 2].clamp(0, w-1)  # x2\n","    boxes[:, 1] = boxes[:, 1].clamp(0, h-1)  # y1\n","    boxes[:, 3] = boxes[:, 3].clamp(0, h-1)  # y2\n","    # ensure x2>=x1, y2>=y1\n","    boxes[:, 2] = torch.maximum(boxes[:, 2], boxes[:, 0])\n","    boxes[:, 3] = torch.maximum(boxes[:, 3], boxes[:, 1])\n","    return boxes\n","\n","def _to_yolo_lines(boxes, labels, w, h):\n","    \"\"\"boxes: tensor [N,4] in xyxy pixel coords; returns list of 'c cx cy w h' normalized strings\"\"\"\n","    if boxes.numel() == 0:\n","        return []\n","    cx = (boxes[:, 0] + boxes[:, 2]) / 2.0 / w\n","    cy = (boxes[:, 1] + boxes[:, 3]) / 2.0 / h\n","    bw = (boxes[:, 2] - boxes[:, 0]) / w\n","    bh = (boxes[:, 3] - boxes[:, 1]) / h\n","    lines = []\n","    for i in range(len(labels)):\n","        c = int(labels[i])\n","        lines.append(f\"{c} {cx[i].item():.6f} {cy[i].item():.6f} {bw[i].item():.6f} {bh[i].item():.6f}\")\n","    return lines\n","\n","def shift_classes_to_zero_index(labels):\n","    labels = labels - 1\n","    return labels\n","\n","def save_image_and_mask(image_t, mask_t, out_image_path, out_mask_path, img_ext=IMG_EXT, mask_ext=MASK_EXT, jpeg_quality=95):\n","    # image_t: C,H,W in [0,1]; mask_t: H,W long\n","    C, H, W = image_t.shape\n","    img = (image_t.clamp(0,1).permute(1,2,0).cpu().numpy() * 255.0).round().astype(np.uint8)\n","    img_pil = Image.fromarray(img)\n","    if img_ext.lower() in [\".jpg\", \".jpeg\"]:\n","        img_pil.save(out_image_path, quality=jpeg_quality, subsampling=0, format=\"JPEG\")\n","    else:\n","        img_pil.save(out_image_path)\n","\n","    mask_arr = mask_t.cpu().numpy().astype(np.uint8)\n","    mask_pil = Image.fromarray(mask_arr)\n","    if mask_ext.lower() in [\".jpg\", \".jpeg\"]:\n","        mask_pil.save(out_mask_path, quality=jpeg_quality, subsampling=0, format=\"JPEG\")\n","    else:\n","        mask_pil.save(out_mask_path)\n","\n","def export_to_yolo(ds, out_dir):\n","    images_dir = Path(out_dir) / \"images\"\n","    labels_dir = Path(out_dir) / \"labels\"\n","    masks_dir  = Path(out_dir) / \"masks\"\n","\n","    total = len(ds)\n","    shifted_any = False\n","    empty_labels = 0\n","    written = 0\n","\n","    for i in tqdm(range(total), desc=\"Converting\"):\n","        image_t, (inst_t, labels_t, boxes_t), stem = ds[i]\n","        # Clamp and filter degenerate boxes\n","        _, H, W = image_t.shape\n","        boxes_t = _clamp_boxes(boxes_t.clone(), W, H)\n","\n","        # drop zero-size boxes\n","        if boxes_t.numel() > 0:\n","            wh = (boxes_t[:, 2] - boxes_t[:, 0]) * (boxes_t[:, 3] - boxes_t[:, 1])\n","            keep = wh > 0\n","            boxes_t = boxes_t[keep]\n","            labels_t = labels_t[keep]\n","\n","        labels_t = shift_classes_to_zero_index(labels_t)\n","\n","        # write image + mask\n","        out_image_path = images_dir / f\"{stem}{IMG_EXT}\"\n","        out_mask_path  = masks_dir  / f\"{stem}{MASK_EXT}\"\n","        save_image_and_mask(image_t, inst_t, out_image_path, out_mask_path, IMG_EXT, MASK_EXT, JPEG_QUALITY)\n","\n","        # write YOLO label file\n","        out_label_path = labels_dir / f\"{stem}.txt\"\n","        lines = _to_yolo_lines(boxes_t, labels_t, W, H)\n","        if len(lines) == 0:\n","            empty_labels += 1\n","            open(out_label_path, \"w\").close()\n","        else:\n","            with open(out_label_path, \"w\") as f:\n","                f.write(\"\\n\".join(lines))\n","        written += 1\n","\n","    print(f\"\\nDone. Wrote {written} samples.\")\n","    print(f\"Empty label files: {empty_labels}\")\n","    if shifted_any:\n","        print(\"Note: Detected 1-indexed classes in .mat and shifted to 0-index for YOLO.\")\n","\n","def zip_dataset(out_dir, dataset_name):\n","    base_dir = Path(out_dir)\n","    zip_path = str(base_dir.parent / f\"{dataset_name}.zip\")\n","    # Remove old zip if exists\n","    if os.path.exists(zip_path):\n","        os.remove(zip_path)\n","    shutil.make_archive(str(base_dir.parent / dataset_name), 'zip', base_dir)\n","    print(\"Zipped dataset at:\", zip_path)\n","    return zip_path\n"],"metadata":{"id":"YQ25OKSb359I","executionInfo":{"status":"ok","timestamp":1759240878217,"user_tz":-180,"elapsed":6,"user":{"displayName":"Alexandru Manole","userId":"16000231996200139655"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# Create dataset and export\n","ds = LizardDataset(IMAGE_ROOT, LABEL_ROOT, debug=False)\n","\n","export_to_yolo(ds, OUT_DIR)\n","zip_path = zip_dataset(OUT_DIR, DATASET_NAME)\n","\n","print(\"All set! You can find it here in Drive:\", zip_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yP34IW613iFN","executionInfo":{"status":"ok","timestamp":1759240909914,"user_tz":-180,"elapsed":30238,"user":{"displayName":"Alexandru Manole","userId":"16000231996200139655"}},"outputId":"ef2b2e31-565f-49d5-bac8-e45d2865020a"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1309866361.py:56: UserWarning: Argument(s) 'value, mask_value' are not valid for transform PadIfNeeded\n","  A.PadIfNeeded(min_height=TARGET_SIZE, min_width=TARGET_SIZE,\n"]},{"output_type":"stream","name":"stdout","text":["/content/dataset/\n","Found 238 images and 238 labels.\n"]},{"output_type":"stream","name":"stderr","text":["Converting:   0%|          | 0/238 [00:00<?, ?it/s]/tmp/ipython-input-1309866361.py:105: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  inst_t = torch.tensor(aug['masks'][0], dtype=torch.long)\n","Converting: 100%|██████████| 238/238 [00:25<00:00,  9.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Done. Wrote 238 samples.\n","Empty label files: 0\n","Zipped dataset at: /content/drive/MyDrive/yolo_lizard_512.zip\n","All set! You can find it here in Drive: /content/drive/MyDrive/yolo_lizard_512.zip\n"]}]}]}